[{"authors":null,"categories":null,"content":" phbp project aims to present base risk estimate and related functions based on Proportional Hazards Models using Bernstein Polynomials (BP). For this, a demo was made making use of larynx dataset available at KMSurv Rpackage. Most relevent results were briefly explained and displayed throughout this text.\nSurvival analysis Bernstein polynomials Adaptive rejection metropolis sampling within gibbs sampling Jacobian Method  Cox Proportional Hazards (PH) model (partial likelihood) First, we fitted gold standard Cox model. Due to avoid overflow in the argument to the exponential function, centered but not scaled data were used, these actions do lead to numerical stability as mentioned at survival Rpackage details. For more about proportional hazards models see Klein and Moeschberger(2006).\nlarynx$age \u0026lt;- scale(larynx$age, scale = F) larynx$stage2 = scale(as.numeric(larynx$stage == 2), scale = F) larynx$stage3 = scale(as.numeric(larynx$stage == 3), scale = F) larynx$stage4 = scale(as.numeric(larynx$stage == 4), scale = F) ## [1] \u0026quot;env0\u0026quot; ## character(0) ## [1] \u0026quot;survival\u0026quot; \u0026quot;KMsurv\u0026quot;  Fitting Proportional Hazards Regression Model: Surv(larynxtime, larynxdelta) ~ age + stage2 + stage3 + stage4     coef exp(coef) se(coef) z p    age 0.01903 1.019 0.01426 1.335 0.182  stage2 0.14 1.15 0.4625 0.3028 0.762  stage3 0.6424 1.901 0.3561 1.804 0.07125  stage4 1.706 5.507 0.4219 4.043 0.00005267    Likelihood ratio test=18.31 on 4 df, p=0.001072 n= 90, number of events= 50\nBernstein polynomials and Log-likelihood function BP is often used to aproximate functions in close intervals $[a,b]$, according to Osman and Gosh (2012), it can be used as well to approximate cumulative hazard functions in $[0, \\tau ]$, such that $\\tau$ is the time which there is no more survivers, i.e. we assume no cure fraction.\nBasic notations, see Osman and Gosh (2012) for more:\n B is the polynomial that approximates the cumulative risk function. It is well known that partial (dot over B) gives us the risk function, see Klein and Moeschberger(2006).\n Quantities in red appeared after applying derivate w/ respect to t. Note that those are given according to the summation index k and from here and forth are parameters to be estimated. In blue there is a beta density over $\\tau$ evaluated at $t/ \\tau$ , denoted by g.\n Getting H back consists of accumulating hazard function $h()$ from 0 to t. This ends up following beta cumulative density eval at $t/ \\tau$, after a rearrangement trick.\n We used b and B for polynomials basis instead of g and G on code.\n  function (time, m = 0, tau = 0) { n \u0026lt;- length(time) if (sum(time \u0026gt;= 0) != n | m \u0026lt; 0 | tau \u0026lt; 0) { return(NA) break } if (tau == 0) { tau \u0026lt;- max(time) } if (m == 0) { m \u0026lt;- ceiling(sqrt(length(time))) } k \u0026lt;- 1:m b \u0026lt;- matrix(NA, n, m) B \u0026lt;- matrix(NA, n, m) y \u0026lt;- time/tau for (i in 1:n) { for (k in 1:m) { b[i, k] \u0026lt;- dbeta(y[i], k, m - k + 1)/tau B[i, k] \u0026lt;- pbeta(y[i], k, m - k + 1) } } return(list(b = b, B = B, m = m, tau = tau)) }  Above one could use sapply function instead of nested for loops. However, I opted to make a more intelligible code. Unlike the Cox model, the proposed approach does not use partial likelihood.\nfunction (par, m, delta, Z, b, B) { gamma \u0026lt;- abs(matrix(par[1:m], ncol = 1)) beta \u0026lt;- matrix(par[(m + 1):(m + q)], ncol = 1) q \u0026lt;- length(par) - m eta \u0026lt;- Z %*% beta h \u0026lt;- b %*% gamma H \u0026lt;- B %*% gamma res \u0026lt;- sum(delta * log(h * exp(eta)) - H * exp(eta)) return(res) }  Classical inference was made using BFGS numeric optimization to find the argument (gamma, beta) that maximizes log-likelihood.\nProportional hazards model related functions using BP Curves derived from gamma vector estimates obtained with clas$par.\nBayesian Approach Bayesian inference wasn\u0026rsquo;t as straight forward as using numerical methods already implemented in R. But it was elegantly done based on gibbs sampling and Adaptive Rejection Metropolis Sampling (ARMS) algorithm. In general, after sampling directly from posterior distribution, Bayesian estimators are calculated for each iteration risk curve.\nHowever, when sampling from multivariate posterior is not possible, it is usual to sample from posterior conditionals distributions on Gibbs sampling. Even if sampling from conditionals is not possible, there are some alternatives, we recurred to Adaptive Rejection Metropolis Sampling (ARMS) within Gibbs. One can\u0026rsquo;t know, but ARMS requires a grid delimiting the aimed density support, we used HI package arms function and Jacobian variable transformation to push all variables in [0,1]. See whole Rscript at gibbs.R repository file.\n# priors parameters (hyperparameters) a_gammak = .01 b_gammak = .01 a_gammak / b_gammak  ## [1] 1  a_gammak / (b_gammak)^2  ## [1] 100  m_beta \u0026lt;- rep(0, q) S_beta \u0026lt;- diag(100, q, q) it \u0026lt;- 1e4 begtime \u0026lt;- Sys.time() source(\u0026quot;gibbs.R\u0026quot;) library(coda) Sys.time() - begtime  ## Time difference of 58.96 secs  burnin \u0026lt;- .5 * it par_mcmc \u0026lt;- as.mcmc(par_samp[burnin:it, ]) par(mfrow = c(4,4)) traceplot(par_mcmc) par(mfrow = c(4,4))  Estimates using BP       lower upper mean median SD    gamma1 0.09565 0 0.2332 0.1069 0.1018 0.0732  gamma2 -0.236 0 0.3944 0.159 0.1374 0.133  gamma3 0.000006972 0 0.3095 0.088 0.0441 0.1045  gamma4 0.07928 0 0.3754 0.1042 0.0505 0.1239  gamma5 0.2653 0 0.4719 0.1412 0.0779 0.1581  gamma6 0.00003381 0 0.727 0.2322 0.1412 0.2434  gamma7 0.7544 0 0.8949 0.2924 0.1966 0.2875  gamma8 0.00001158 0 0.9374 0.2941 0.1854 0.3083  gamma9 -0.000005073 0 0.8511 0.2468 0.145 0.2968  gamma10 0.000002499 0 0.9865 0.2772 0.1564 0.3613  beta1 0.01927 -0.008 0.0468 0.0201 0.0198 0.0141  beta2 0.1719 -0.7233 1.071 0.159 0.1702 0.4553  beta3 0.6585 -0.0427 1.309 0.6505 0.6472 0.3449  beta4 1.799 0.9948 2.668 1.795 1.805 0.4319    Base cumulative risk and base survival curves Survival curves by groups for a given 77 years old patient Bayesian using Rstan (No-U-Turn Sampler) Stan is a state-of-the-art platform for statistical modeling, an alternative to Gibbs Sampling seen above.\n# Clear the R workspace and load rstan. # rm(list=ls(all=TRUE)) library(rstan) # avoid recompilations rstan_options(auto_write = TRUE) # run different chains in parallel. options(mc.cores = parallel::detectCores()) # Prepare data for Stan data = list(n = n, m = m, q = q, delta = larynx$delta, Z = Z, B = base$B, b = base$b, a_gammak = a_gammak, b_gammak = b_gammak, m_beta = m_beta[1], S_beta = S_beta[1]) pars = c(\u0026quot;beta\u0026quot;,\u0026quot;gamma\u0026quot;) initvals = \u0026quot;random\u0026quot; iter = it # total iterations (including warm-up). warmup = it * .5 chains = 1 begtime \u0026lt;- Sys.time() output = stan(file = \u0026quot;bern.stan\u0026quot;, data=data, iter=iter, warmup=warmup, chains=chains, pars=pars, init=initvals, verbose=FALSE)  ## ## SAMPLING FOR MODEL 'bern' NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 5.1e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.51 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 10000 [ 0%] (Warmup) ## Chain 1: Iteration: 1000 / 10000 [ 10%] (Warmup) ## Chain 1: Iteration: 2000 / 10000 [ 20%] (Warmup) ## Chain 1: Iteration: 3000 / 10000 [ 30%] (Warmup) ## Chain 1: Iteration: 4000 / 10000 [ 40%] (Warmup) ## Chain 1: Iteration: 5000 / 10000 [ 50%] (Warmup) ## Chain 1: Iteration: 5001 / 10000 [ 50%] (Sampling) ## Chain 1: Iteration: 6000 / 10000 [ 60%] (Sampling) ## Chain 1: Iteration: 7000 / 10000 [ 70%] (Sampling) ## Chain 1: Iteration: 8000 / 10000 [ 80%] (Sampling) ## Chain 1: Iteration: 9000 / 10000 [ 90%] (Sampling) ## Chain 1: Iteration: 10000 / 10000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 98.5036 seconds (Warm-up) ## Chain 1: 138.742 seconds (Sampling) ## Chain 1: 237.245 seconds (Total) ## Chain 1:  Sys.time()-begtime  ## Time difference of 3.981 mins  traceplot(output, pars=c(\u0026quot;beta\u0026quot;,\u0026quot;gamma\u0026quot;))        lower upper mean median SD    gamma1 0.09565 0 0.0725 0.0087 0 0.0341  gamma2 -0.236 0.1786 0.5698 0.3782 0.38 0.0996  gamma3 0.000006972 0 0.0014 0.0022 0 0.0171  gamma4 0.07928 0 0.003 0.0041 0 0.0269  gamma5 0.2653 0 0.1752 0.0294 0 0.1303  gamma6 0.00003381 0 1.183 0.5155 0.6279 0.4486  gamma7 0.7544 0 1.412 0.3649 0 0.5334  gamma8 0.00001158 0 0.0965 0.0437 0 0.2267  gamma9 -0.000005073 0 0.0047 0.0123 0 0.1016  gamma10 0.000002499 0 0.0046 0.0109 0 0.1031  beta1 0.01927 -0.0074 0.0483 0.0193 0.0192 0.0142  beta2 0.1719 -0.8207 1.054 0.1133 0.1259 0.482  beta3 0.6585 0.015 1.415 0.6704 0.6643 0.3555  beta4 1.799 0.9267 2.647 1.808 1.815 0.4369    Future Work Investigate and deal with suspicion of cure fraction present in dataset and explore the use of BP for Cox model extensions. In addition to that, a Rpackage will be released to wrap a wide variety of survival models using BP by the end of the year as part of my MSc final project.\nReferences  Giolo, S. R., \u0026amp; Colosimo, E. A. (2006). Análise de sobrevivência aplicada. Edgard Blucher.\n Ibrahim, J. G., Chen, M. H., \u0026amp; Sinha, D. (2001). Bayesian survival analysis. Springer Science \u0026amp; Business Media.\n Klein, J. P., \u0026amp; Moeschberger, M. L. (2006). Survival analysis: techniques for censored and truncated data. Springer Science \u0026amp; Business Media.\n Lorentz, G. G. (2012). Bernstein polynomials. American Mathematical Soc..\n Osman, M., \u0026amp; Ghosh, S. K. (2012). Nonparametric regression models for right-censored data using Bernstein polynomials. Computational Statistics \u0026amp; Data Analysis, 56(3), 559-573.\n  ","date":1546619040,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546619040,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2019-01-04T14:24:00-02:00","relpermalink":"/project/internal-project/","section":"project","summary":"Semiparametric Survival Modelling via Bernstein Polynomials.","tags":["Survival Analysis","Stan","Bayesian","Bernstein polynomials"],"title":"phbp","type":"project"},{"authors":null,"categories":null,"content":"All content available on the website is freely accessible, authored or co-authored by independent writers. Therefore, all the material is original and was produced with the intention of collaborating with the scientific community. It is not allowed to reproduce the content and to publish it, the use of the intellectual work must be rewarded according to Brazilian Law No. 9,610 of 1998. New collaborators and partners are welcome.\n","date":1546612080,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546612080,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"2019-01-04T12:28:00-02:00","relpermalink":"/privacy/","section":"","summary":"All content available on the website is freely accessible, authored or co-authored by independent writers. Therefore, all the material is original and was produced with the intention of collaborating with the scientific community. It is not allowed to reproduce the content and to publish it, the use of the intellectual work must be rewarded according to Brazilian Law No. 9,610 of 1998. New collaborators and partners are welcome.","tags":null,"title":"Intellectual Property and Privacy Policy","type":"page"},{"authors":["RV Panaro"],"categories":null,"content":" Índice de Preços ao Consumidor Amplo A produção do IPCA origina de 3 pesquisas longitudinais.\n Pesquisa de Orçamentos Familiares (POF): Ponderação das populações-objetivo.\n Pesquisa de Locais de Compra (PLC): Cadastro de informantes da pesquisa.\n Pesquisa de Especificação de Produtos e Serviços (PEPS): Produtos e serviços pesquisados.\n  O índice leva em consideração 465 sub-itens classificados em 8 grupos.\nGrupos\n\n alimentação e bebidas,\n artigos de residência,\n transportes,\n comunicação,\n despesas pessoais,\n habitação,\n saúde e\n cuidados pessoais, vestuário e educação.\n  Objetivo O objetivo geral deste trabalho é extrair inferências de séries temporais considerando a abordagem clássica de Box-Jenkins (1976).\n Selecionar modelo teórico para representar os dados,\n Estimar o modelo através de seus parâmetros, verificar a qualidade do ajuste e usar o modelo ajustado para melhor compreensão do mecanismo que gera a série,\n Prever valores futuros da série, de h passos à frente, através desses modelos, considerando o período total observado ao final da série histórica.\n Comparar a previsão dos modelos e verificar a existência de um melhor modelo com base nas medidas de erro de previsão.\n  Referências [1] MORETTIN, P. A.; TOLOI, C. Análise de séries temporais. [S.l.]: Blucher, 2006.\n[2] OECD. Statistics, Knowledge and Policy: Key Indocators to Inform Decision Making. [S.l.]: OECD Publishing, 1996.\n[3] IBGE. Sistema Nacional de Preços ao Consumidor: Métodos de Cálculo. [S.l.]: Série Relatórios Metodológicos, Rio e Janeiro: 2006.\n[4] SENRA, N. et al. História das estatísticas brasileiras. [S.l.]: Ministério do Planejamento, Orçamento e Gestão, Instituto Brasileiro de Geografia e Estatística-IBGE, Centro de Documentação e Disseminação de Informações, 2009.\n[5] QUADROS, S. et al. Índices de preços: História, fórmulas e divergências. Revista Conjuntura Econômica, v. 45, n. 3, p. 36-37, 1991.\n[6] LOURENÇO, G. M.; ROMERO, M. Indicadores econômicos. FAE BUSINESS SCHOOL. Economia empresarial. Curitiba: Associação Franciscana de Ensino Senhor Bom Jesus, p. 27-41, 2002.\n[7] IBGE. Disponível em: http://www. ibge. gov. br. Acesso em, v. 12, 2017.\n[8] FGV. Disponível em: http://www. portal. fgv. br. Acesso em, v. 12, 2017.\n[9] FIPE. Disponível em: http://www. fipe. org. br. Acesso em, v. 12, 2017.\n[10] LUQUE, C. A.; VASCONCELLOS, M. A. S. d. Considerações sobre o problema da inflação. Manual de economia, v. 5, p. 336-51, 2002.\n[11] SERIGATI, F. C.; POSSAMAI, R. Inflação de demanda. AgroANALYSIS, v. 34, n. 05, p. 13-15.\n[12] LOPES, F. L. et al. Inflação inercial, hiperinflação e desinflação: notas e conjecturas. Pontifícia Universidade Católica do Rio de Janeiro, 1984.\n[13] BRASIL. Decreto n.º 3.008, de 21 de junho de 1999. Diário Oficial da União, Seção 1, p. 4, 1999. [14] INMAN, P. Brazil\u0026rsquo;s economy overtakes UK to become world\u0026rsquo;s sixth largest. Guardian Newspaper. 2012.\n[15] TREVISANI, P.; JELMAYER, R. Brazil\u0026rsquo;s dilma roussef defends austerity measures in tv address. The Wall Street Journal, 2015.\n[16] CARTA, D. C. Rio de janeiro: Ipea, n. 37, out. 2017.\n[17] CARTA, D. C. Rio de janeiro: Ipea, n. 36, jul. 2017.\n[18] BCB. Resolução n.º 4.499, de 30 de junho de 2016. Diário Oficial da União, Seção 1, p. 55, 2016. Referências 49\n[19] BCB. Resolução n.º 4.582, de 29 de junho de 2017. Diário Oficial da União, Seção 1, p. 37, 2017.\n[21] FARIAS, A. M. L. de; LAURENCEL, L. da C. Números Índices. 2015.\n[22] BROCKWELL, P. J.; DAVIS, R. A. Time series: theory and methods. [S.l.]: Springer Science \u0026amp; Business Media, 2013.\n[20] SIDRA, I. Disponível em: www. sidra. ibge. gov. br. [S.l.]: Rio de Janeiro, 2017.\n[23] AKAIKE, H. Information theory and an extension of the maximum likelihood principle. In: Selected papers of hirotugu akaike. [S.l.]: Springer, 1998. p. 199-213.\n[24] EMILIANO, P. C. et al. Critérios de informação de akaike versus bayesiano: análise comparativa. 19 o Simpósio Nacional de Probabilidade e Estatística, 2010.\n[25] BOX, G. E. et al. Time series analysis: forecasting and control. [S.l.]: John Wiley \u0026amp; Sons, 1970.\n[26] DICKEY, D. A.; FULLER, W. A. Distribution of the estimators for autoregressive time series with a unit root. Journal of the American statistical association, Taylor \u0026amp; Francis, v. 74, n. 366a, p. 427-431, 1979.\n[27] JARQUE, C. M.; BERA, A. K. Efficient tests for normality, homoscedasticity and serial independence of regression residuals. Economics letters, Elsevier, v. 6, n. 3, p. 255-259, 1980.\n[28] ARTUSI, R.; VERDERIO, P.; MARUBINI, E. Bravais-pearson and spearman correlation coefficients: meaning, test of hypothesis and confidence interval. Int J Biol Markers, v. 17, n. 148-151. PMID, p. 12113584, 2002.\n[29] LJUNG, G. M.; BOX, G. E. On a measure of lack of fit in time series models. Biometrika, Oxford University Press, v. 65, n. 2, p. 297-303, 1978.\n[30] DEBATES, C. de; REAL, A. de P.; PEIXOTO, A. d. S. O plano real. Assembleia Legislativa do Estado de Minas Gerais, 2004.\n[31] HARRISON, J.; WEST, M. Bayesian forecasting \u0026amp; dynamic models. [S.l.]: Springer New York City, 1999.\n","date":1515783780,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515783780,"objectID":"88216c2af22d68b4b3af20384548b2ed","permalink":"/publication/undergraduate-term/","publishdate":"2018-01-12T17:03:00-02:00","relpermalink":"/publication/undergraduate-term/","section":"publication","summary":"A inflação é o aumento do nível do preços de um determindo produto ao longo do tempo. A alta inflação é prejudicial para a economia de um país e, quando fora de controle, pode gerar diversos problemas, tais como, a desvalorização da moeda nacional e aumento dos preços de produtos importados, a diminuição dos investimentos no setor produtivo, aumento da especulação financeira, elevação da taxa de juros e aumento do desemprego, além do clima de instabilidade econômica e insatisfação popular. O objetivo desse trabalho consiste em avaliar a série histórica do IPCA, considerando os índices produzidos entre dezembro de 1999 e outubro de 2017 (inclusive) e identificar, estimar e comparar modelos. Para tanto, foram depurados dados mensais do Sistema IBGE de Recuperação Automática (SIDRA) referentes ao IPCA acumulado anual. O foco do estudo é modelar e produzir estimativas para valores futuros do IPCA e, dessa forma, identificar padrões e tendências presentes na economia do Brasil por meio de modelos que serão comparados em termos de parcimônia, ajuste e qualidade das previsões usando modelos Sazonais Autoregressivos Integrados de Médias Móveis (SARIMA).","tags":["autoregressive","SARIMA","time series","classical inference"],"title":"Modelagem e Previsão da Inflação Anual Efetiva medida pelo IPCA","type":"publication"},{"authors":null,"categories":null,"content":"","date":1515081120,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515081120,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2018-01-04T13:52:00-02:00","relpermalink":"/project/external-project/","section":"project","summary":"This project refers to the collaboration project (@scp93 and @Rumenick) that disseminates basic concepts about R programming language, external link [rn00bs.com.br](http://rn00bs.com.br/).","tags":["R","basic statistics","beginners","n00bs"],"title":"R para n00bs","type":"project"},{"authors":null,"categories":null,"content":" EDUCATION Universidade Federal de Minas Gerais - UFMG, M.S. Statistics 2018–current\nInterests: Bayesian survival analysis and statistical computing\nUniversidade Federal Fluminense - UFF, B.S. Statistics 2013–2017\nModeling and Forecasting of Annual Effective Inflation (portuguese translated) CR 7.6/10\nUniversity of Wisconsin Platteville - UWP, Brazilian Scientific Mobility Program 2014–2015\nMathematics (ABET Accredited) GPA 3.4/4.0\n EMPLOYMENT Business Modelling Project (UFMG):\n Research Fellow 2018–current\n Documentation Active \u0026amp; Intelligent Design Laboratory (UFF):\n Intern, statistical support in case studies 2017\n Grupo de Planejamento e Pesquisa - GPP:\n Intern, coding and results reporting on market and opinion researches 2016–2018\n  LANGUAGES Native portuguese, fluent English and comprehensive Spanish\n SKILLS R\n Markdown, HUGO, dplyr, etc.\n Python\n pytorch\n LaTeX\n complete articles and reports\n Statistical Software System\n proc freq, proc print and other procedures\n MS Office\n pivot tables\n  AWARDS 2014 Brazilian Scientific Mobility Program Awarded one year scholarship at UWP granted by CAPES.\n REFERENCES Vinícius Mayrink, Ph.D.\nStatistics Department\nFederal University of Minas Gerais\nICEx\nAv. Antônio Carlos 6627\nPampulha Campus\nBelo Horizonte, MG 31270-901\n+55 31- 3409-5900\nvdinizm@gmail.com\nAna Beatriz Fonseca, Ph.D.\nStatistics Department\nFluminense Federal University\nMathematics and Statistics Institute\nRua Prof. Marcos Waldemar de Freitas Reis Niterói, RJ 24210-201\n+55 21-2629-2005\nabmfonseca@id.uff.br\nKristin Dalby, ESL Coordinator\nInternational Programs Office\nUniversity of Wisconsin – Madison\n716 Langdon St #301\nMadison, WI 53706\n+1 608-265-6329\nkdalby@wisc.edu\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b0de48e36a88e094d3db6464277084bb","permalink":"/cv/cv/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/cv/cv/","section":"CV","summary":"EDUCATION Universidade Federal de Minas Gerais - UFMG, M.S. Statistics 2018–current\nInterests: Bayesian survival analysis and statistical computing\nUniversidade Federal Fluminense - UFF, B.S. Statistics 2013–2017\nModeling and Forecasting of Annual Effective Inflation (portuguese translated) CR 7.6/10\nUniversity of Wisconsin Platteville - UWP, Brazilian Scientific Mobility Program 2014–2015\nMathematics (ABET Accredited) GPA 3.4/4.0\n EMPLOYMENT Business Modelling Project (UFMG):\n Research Fellow 2018–current\n Documentation Active \u0026amp; Intelligent Design Laboratory (UFF):","tags":null,"title":"CV","type":"CV"}]